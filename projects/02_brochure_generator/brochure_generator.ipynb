{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ðŸ“„ Brochure Generator\n\nAn AI-powered tool that automatically creates professional company brochures by analyzing websites and extracting relevant content using advanced web scraping and natural language processing.\n\n## ðŸ“‹ Overview\n\nThis project intelligently crawls company websites, identifies and extracts relevant content from multiple pages (About, Products, Contact, etc.), and generates polished marketing brochures using AI language models. The tool leverages OpenAI's GPT-4o-mini to create engaging, professional content suitable for potential customers, investors, and job seekers.\n\n## âœ¨ Key Features\n\n- **ðŸ¤– AI-Powered Content Generation**: Uses OpenAI's `gpt-4o-mini` for intelligent brochure writing\n- **ðŸ•·ï¸ Smart Web Crawling**: Automatically identifies and crawls relevant company website pages\n- **ðŸŽ¯ Intelligent Link Selection**: Uses AI to filter and select only brochure-relevant links (About, Products, Contact, etc.)\n- **ðŸ“ Professional Output**: Generates clean, marketing-ready content in Markdown format\n- **ðŸ”— Multi-Page Processing**: Combines content from multiple website pages for comprehensive brochures\n- **ðŸš€ Automated Workflow**: End-to-end automation from URL input to finished brochure\n\n## ðŸ› ï¸ Technology Stack\n\n| Component | Technology | Purpose |\n|-----------|------------|---------|\n| **AI Model** | OpenAI GPT-4o-mini | Content generation and link analysis |\n| **Web Scraping** | BeautifulSoup + Requests | Website content extraction |\n| **JSON Processing** | Python JSON | AI response parsing |\n| **Content Processing** | Custom classes | Website data management |\n| **Output Format** | Markdown | Professional document formatting |\n| **Language** | Python | Core development |\n\n## ðŸš€ Installation Requirements\n\n### Python Dependencies\n```bash\npip install requests beautifulsoup4 openai python-dotenv\n```\n\n### Environment Variables\n- `OPENAI_API_KEY` - Required for AI content generation\n\n## ðŸŽ¯ Project Scope\n\n- âœ… **Multi-Page Content**: Processes main page and relevant sub-pages\n- âœ… **AI Link Selection**: Intelligently identifies brochure-relevant links\n- âœ… **Content Synthesis**: Combines multiple pages into cohesive brochure\n- âœ… **Professional Output**: Marketing-ready brochure content\n- âœ… **Automated Processing**: Minimal manual intervention required\n- âŒ **Visual Design**: Focuses on content, not visual layout\n\n## ðŸ† Skill Level\n\n**Intermediate** - Perfect for developers learning:\n- AI-assisted content creation\n- Multi-step web scraping workflows\n- JSON parsing and data processing\n- OpenAI API integration\n- Content synthesis techniques\n\n## ðŸš€ Use Cases\n\n- **ðŸ¢ Company Brochures**: Generate marketing materials for businesses\n- **ðŸ“ˆ Sales Materials**: Create compelling company overviews\n- **ðŸ’¼ Investor Presentations**: Extract key company information\n- **ðŸŽ¯ Marketing Automation**: Automated content creation pipelines\n- **ðŸ“Š Competitive Analysis**: Analyze competitor websites\n\n## ðŸ’¡ Benefits\n\n- **â° Time-Saving**: Reduces brochure creation time by 90%\n- **ðŸŽ¯ Professional Quality**: AI-generated marketing content\n- **ðŸ“± Consistent Format**: Standardized brochure structure\n- **ðŸ”„ Scalable**: Process multiple companies efficiently\n- **ðŸ¤ Marketing-Ready**: Professional output for immediate use\n\n## ðŸ”§ Core Components\n\n- **`WebSite`**: Data class for website information\n- **`WebUrlCrawler`**: Web scraping and content extraction\n- **`LLMClient`**: OpenAI API integration\n- **`BrochureGenerator`**: Main orchestration and content synthesis\n\n---\n\n*This project demonstrates advanced AI integration for automated marketing content creation, combining web scraping, natural language processing, and content generation.*",
   "id": "fb811389364f686b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T02:48:03.761325Z",
     "start_time": "2025-08-10T02:48:03.367873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# System & Environment\n",
    "# ===========================\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, Markdown, update_display\n",
    "from openai import OpenAI"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Scraping Module",
   "id": "d6271714d125a7f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:26.801970Z",
     "start_time": "2025-08-10T02:55:26.797193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def _setup_driver(self):\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.set_page_load_timeout(self.timeout)\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Failed to initialize Chrome driver: {e}\")\n",
    "\n",
    "    def _extract_main_content(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'img', 'input', 'button', 'nav', 'footer', 'header']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "\n",
    "        # Try to find main content containers in order of preference\n",
    "        content_selectors = [\n",
    "            'main',\n",
    "            'article',\n",
    "            '[role=\"main\"]',\n",
    "            '.content',\n",
    "            '#content',\n",
    "            '.main-content',\n",
    "            '#main-content'\n",
    "        ]\n",
    "\n",
    "        for selector in content_selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                return content_element.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        # Fallback to body if no main content container found\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return body.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        return soup.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    def crawl(self, url):\n",
    "        if not self.driver:\n",
    "            self._setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "\n",
    "            WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            html_content = self.driver.page_source\n",
    "            main_content = self._extract_main_content(html_content)\n",
    "            return main_content\n",
    "\n",
    "        except TimeoutException:\n",
    "            raise Exception(f\"Timeout while loading {url}\")\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ],
   "id": "986d6161971d104b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:30.816361Z",
     "start_time": "2025-08-10T02:55:30.811533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "class WebSite:\n",
    "    def __init__(self, url, title, body, links):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.links = links\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    # some websites need to use proper headers when fetching them\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def crawl(self, url) -> WebSite:\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            body = soup.body.get_text(strip=True, separator='\\n')\n",
    "        else:\n",
    "            body = \"\"\n",
    "\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        links = [link for link in links if link]\n",
    "\n",
    "        return WebSite(url, title, body, links)\n"
   ],
   "id": "a25bf608fed446e2",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM client",
   "id": "eea901241c283b8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:36.665188Z",
     "start_time": "2025-08-10T02:55:36.662190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model, base_url=None):\n",
    "        self.model = model\n",
    "        if base_url:\n",
    "            self.openai = OpenAI(base_url=base_url, api_key=model)\n",
    "        else:\n",
    "            self.openai = OpenAI()\n",
    "\n",
    "    def generate_text(self, user_prompt, system_prompt=\"\") -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages= messages,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ],
   "id": "30bb98c07c4b7e45",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Brochure Creation",
   "id": "8dcc16eb5f449542"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:38.834210Z",
     "start_time": "2025-08-10T02:55:38.830005Z"
    }
   },
   "cell_type": "code",
   "source": "class BrochureGenerator:\n    \"\"\"\n    An AI-powered brochure generator that creates professional company brochures\n    by analyzing websites and extracting relevant content.\n    \n    This class orchestrates the entire brochure generation process:\n    1. Crawls the main company website\n    2. Uses AI to identify relevant sub-pages (About, Products, Contact, etc.)\n    3. Scrapes content from selected pages\n    4. Combines all content into a cohesive professional brochure\n    \n    The generator focuses on creating marketing materials suitable for potential\n    customers, investors, and job seekers by extracting company culture, products,\n    and career information from the website.\n    \n    Attributes:\n        brochure_url (str): The main URL of the company website to process\n        company_name (str): The name of the company for brochure personalization\n        crawler (WebUrlCrawler): Web scraping client for content extraction\n        main_webpage (WebSite): The scraped main website content and metadata\n        llm_client (LLMClient): AI client for content generation and link analysis\n    \"\"\"\n\n    def __init__(self, llm_client, url, company_name):\n        \"\"\"\n        Initialize the brochure generator with company information and AI client.\n        \n        Args:\n            llm_client (LLMClient): An initialized LLM client for AI operations\n            url (str): The main URL of the company website to process\n            company_name (str): The name of the company for brochure personalization\n        \"\"\"\n        self.brochure_url = url\n        self.company_name = company_name\n        self.crawler = WebUrlCrawler(headless=True)\n        self.main_webpage = self.crawler.crawl(self.brochure_url)\n        self.llm_client = llm_client\n\n    def generate(self) -> str:\n        \"\"\"\n        Generate a complete company brochure by processing website content.\n        \n        This method orchestrates the full brochure generation workflow:\n        1. Analyzes the main webpage links to identify relevant sub-pages\n        2. Scrapes content from selected relevant pages\n        3. Combines all content into a comprehensive text corpus\n        4. Generates a professional brochure using AI content synthesis\n        \n        Returns:\n            str: A complete company brochure in Markdown format, ready for\n                 use in marketing materials, presentations, or publications\n                 \n        Raises:\n            Exception: If web scraping fails for any of the target pages\n            OpenAIError: If AI processing fails during link analysis or content generation\n            json.JSONDecodeError: If the AI returns invalid JSON for link selection\n        \"\"\"\n        links_json = self._get_relevant_links()\n        links = json.loads(links_json)\n        content = self.main_webpage.body\n\n        for link in links['links']:\n            linked_website = self.crawler.crawl(link['url'])\n            content += f\"\\n\\n{link['type']}:\\n\"\n            content += linked_website.body\n\n        return self._get_brochure_body(content=content)\n\n    def _get_relevant_links(self) -> str:\n        \"\"\"\n        Use AI to identify and filter relevant website links for brochure content.\n        \n        This method analyzes all links found on the main webpage and uses AI\n        to select only those that are relevant for a professional brochure.\n        It excludes utility links like login, terms of service, and privacy\n        policies while focusing on content-rich pages like About, Products,\n        Contact, and Company information.\n        \n        Returns:\n            str: A JSON string containing an array of selected relevant links\n                 with their types (about, product, contact, etc.) and URLs\n                 \n        Raises:\n            OpenAIError: If the AI model request fails or returns an error\n        \"\"\"\n        system_prompt = \"\"\"\n        You are given a list of links from a company website.\n        Select only relevant links for a brochure (About, Company, Careers, Products, Contact).\n        Exclude login, terms, privacy, and emails.\n\n        ### **Instructions**\n        - Return **only valid JSON**.\n        - **Do not** include explanations, comments, or Markdown.\n        - Example output:\n        {\n            \"links\": [\n                {\"type\": \"about\", \"url\": \"https://company.com/about\"},\n                {\"type\": \"contact\", \"url\": \"https://company.com/contact\"},\n                {\"type\": \"product\", \"url\": \"https://company.com/products\"}\n            ]\n        }\n        \"\"\"\n\n        user_prompt = f\"\"\"\n        Here is the list of links on the website of {self.main_webpage.url}:\n        Please identify the relevant web links for a company brochure. Respond in JSON format.\n        Do not include login, terms of service, privacy, or email links.\n        Links (some might be relative links):\n        {', '.join(self.main_webpage.links)}\n        \"\"\"\n\n        return self.llm_client.generate_text(system_prompt=system_prompt, user_prompt=user_prompt)\n\n    def _get_brochure_body(self, content) -> str:\n        \"\"\"\n        Generate the final brochure content using AI synthesis of scraped website data.\n        \n        This method takes the combined content from all relevant website pages\n        and uses AI to create a cohesive, professional brochure. The generated\n        content is optimized for multiple audiences including potential customers,\n        investors, and job seekers.\n        \n        Args:\n            content (str): Combined text content from all scraped website pages\n            \n        Returns:\n            str: A professional company brochure in Markdown format with\n                 engaging content about the company's products, culture,\n                 customers, and career opportunities\n                 \n        Raises:\n            OpenAIError: If the AI model request fails during brochure generation\n        \"\"\"\n        system_prompt = \"\"\"\n        You are an expert at writing engaging company brochures. Your task is to read content from a a provided company website and write a short, professional and engaging brochure for potential customers, investors, and job seekers. Include details about the company's culture, customers, and career opportunities if available. Respond in Markdown format.\n        \"\"\"\n        user_prompt = f\"\"\"\n        Create a brochure for '${self.company_name}' using the following content:\n        ${content}\n        \"\"\"\n\n        return self.llm_client.generate_text(system_prompt=system_prompt, user_prompt=user_prompt)",
   "id": "f883745abe603777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Brochure with gpt-4o-mini",
   "id": "8ee04181b63607e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load open_api_key",
   "id": "956694db2b326d70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:42.242683Z",
     "start_time": "2025-08-10T02:55:42.238542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"âœ… API key loaded successfully!\")"
   ],
   "id": "665b5eff290f389f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### configure LLMClient",
   "id": "f168e5b33cd61d9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:55:44.390323Z",
     "start_time": "2025-08-10T02:55:44.372567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_open_ai = \"gpt-4o-mini\"\n",
    "open_ai_llm_client = LLMClient(model=model_open_ai)"
   ],
   "id": "a31069ef237fed85",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:56:09.416199Z",
     "start_time": "2025-08-10T02:55:47.560184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "brochure_generator = BrochureGenerator(llm_client=open_ai_llm_client, company_name='Anthopic', url='https://www.anthropic.com/claude' )\n",
    "brochure_content = brochure_generator.generate()\n",
    "display(Markdown(brochure_content))"
   ],
   "id": "738fc97527ec0916",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# Anthropic: Building the Future of Safe AI\n\nWelcome to Anthropic, where we believe in making AI systems that are safe, reliable, and beneficial for humanity. As a leading AI safety and research company, we are dedicated to pioneering trustworthy AI technologies and creating solutions that enhance the way people interact with technology. Explore our offerings and learn about our commitment to building a brighter future through responsible AI.\n\n## Meet Claude: Your Intelligent Partner\n\n**Claude** is our flagship AI system designed to enhance productivity for individuals and teams. Whether you're tackling coding challenges, conducting research, or developing creative projects, Claude connects seamlessly to your world, offering personalized support that amplifies your capabilities.\n\n- **Reliable Assistance**: Handle complex questions and tasks with clear, step-by-step guidance.\n- **Creative Solutions**: Transform initial ideas into polished, practical outputs.\n- **Collaborative Tools**: Work smarter, whether it's for education, business, or personal development.\n\n## Our Commitment to Safety and Transparency\n\nAt Anthropic, we prioritize safety as a science. Our interdisciplinary teamâ€”comprising researchers, engineers, and policy expertsâ€”works together to ensure that our AI systems are not only cutting-edge but also secure and interpretable. With a focus on transparency, we actively share our research findings and safety practices with the public to foster a culture of accountability in the AI industry.\n\n### Our Core Values\n1. **Act for the Global Good**: We commit to outcomes that benefit humanity in the long run.\n2. **Hold Light and Shade**: We acknowledge the potential risks and rewards of AI development.\n3. **Be Good to Our Users**: Cultivating kindness and generosity in every interaction.\n4. **Ignite a Race to the Top on Safety**: We aim to lead in AI safety measures collectively.\n5. **Do the Simple Thing That Works**: Focusing on pragmatic solutions over complexity.\n6. **Be Helpful, Honest, and Harmless**: Promoting a culture of trust and open communication.\n7. **Put the Mission First**: Our mission drives us to make impactful decisions together.\n\n## Join Our Growing Team\n\nAnthropic is not just about AI; it's about people. We're actively looking for talented individuals from diverse backgrounds who share our passion for AI safety and innovation. Our inclusive company culture and commitment to employee well-being are reflected in our comprehensive benefits:\n\n- **Health & Wellness**: Competitive health, dental, and vision insurance.\n- **Flexible Paid Time Off**: Supporting work-life balance.\n- **Career Growth Support**: Annual education stipends and professional development opportunities.\n\n### Career Opportunities\nWe welcome candidates with varying experiences, whether you have a traditional machine learning background or come from a different field altogether. Our interview process assesses critical thinking and problem-solving skills rather than only specific industry experience.\n\nExplore our open roles and see how you can contribute to shaping the future of safe AI!\n\n## Letâ€™s Connect\n\nWe invite you to join us on this remarkable journey of harnessing AI for good. Whether youâ€™re a potential customer, investor, or future employee, connect with Anthropic to learn more about our mission and how we can work together toward advancing the responsible use of AI.\n\n**Contact Us**: [Visit our website](https://www.anthropic.com) to explore how you can get involved.\n\n---\n\nTogether, letâ€™s create a future where AI elevates lives and innovations thrive. Welcome to Anthropic!"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}