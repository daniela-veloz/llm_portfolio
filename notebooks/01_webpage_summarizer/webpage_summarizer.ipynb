{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üåê WebPage Summarizer\n\nAn intelligent web content summarization tool that extracts and condenses webpage information using advanced AI models.\n\n**üöÄ [Try it live on Hugging Face Spaces!](https://huggingface.co/spaces/daniela-veloz/webpage-summarizer)**\n\n## üìã Overview\n\nThis project creates concise, structured summaries of web content by leveraging state-of-the-art language models and robust web scraping techniques. The tool supports both cloud-based and local AI models, including OpenAI's GPT-4o-mini and the open-source GPT-OSS:20B model through Ollama, providing flexibility for different deployment scenarios. Perfect for quickly understanding lengthy articles, blog posts, or documentation.\n\n## ‚ú® Key Features\n\n- **ü§ñ Dual AI Models**: Powered by OpenAI's `gpt-4o-mini` and open-source `gpt-oss:20b` through Ollama for high-quality text summarization\n- **üîì Local & Cloud Options**: Choose between cloud-based OpenAI models or run models locally with Ollama\n- **üï∑Ô∏è Advanced Web Scraping**: Uses Selenium to handle both static and dynamic JavaScript-rendered websites\n- **üìù Markdown Output**: Generates clean, formatted summaries in Markdown for easy reading and sharing\n- **üéØ Focused Processing**: Efficiently processes individual webpage URLs without crawling entire sites\n- **‚ö° Multi-Tool Integration**: Combines multiple libraries for robust and reliable content extraction\n\n## üõ†Ô∏è Technology Stack\n\n| Component | Technology | Purpose |\n|-----------|------------|---------|\n| **AI Models** | OpenAI GPT-4o-mini, GPT-OSS:20B | Content summarization |\n| **Web Scraping** | Selenium WebDriver | Dynamic content extraction |\n| **HTML Parsing** | BeautifulSoup | Static content processing |\n| **HTTP Requests** | Python Requests | Basic web requests |\n| **AI Integration** | OpenAI API, Ollama | Model access and inference |\n| **Local AI Runtime** | Ollama | Local model execution |\n| **Language** | Python | Core development |\n\n## üöÄ Installation Requirements\n\n### Ollama Setup\nTo use the GPT-OSS:20B model locally, you need to install Ollama:\n\n1. **Install Ollama**: Visit [ollama.com](https://ollama.com) and download for your platform\n2. **Pull the model**: After installation, run:\n   ```bash\n   ollama pull gpt-oss:20b\n   ```\n3. **Start Ollama service**: The service should start automatically, or run:\n   ```bash\n   ollama serve\n   ```\n\n### Python Dependencies\nInstall required Python packages:\n```bash\npip install selenium beautifulsoup4 webdriver-manager openai requests python-dotenv\n```\n\n## üéØ Project Scope\n\n- ‚úÖ **Single URL Processing**: Focuses on individual webpage content\n- ‚úÖ **Content Extraction**: Handles both static and dynamic web content\n- ‚úÖ **AI Summarization**: Generates intelligent, contextual summaries\n- ‚úÖ **Structured Output**: Provides clean Markdown formatting\n- ‚úÖ **Local & Cloud AI**: Supports both local Ollama and cloud OpenAI models\n- ‚ùå **Site Crawling**: Does not process entire websites or multiple pages\n\n## üèÜ Skill Level\n\n**Beginner-Friendly** - Perfect for developers learning:\n- Web scraping fundamentals\n- AI model integration\n- API consumption\n- Local AI deployment with Ollama\n- Content processing pipelines\n\n## üöÄ Use Cases\n\n- **üì∞ News Article Summaries**: Quickly digest lengthy news articles\n- **üìö Research Papers**: Extract key points from academic content\n- **üìñ Documentation**: Summarize technical documentation\n- **üõçÔ∏è Product Reviews**: Condense detailed product information\n- **üíº Business Reports**: Extract insights from corporate content\n\n## üí° Benefits\n\n- **‚è∞ Time-Saving**: Reduces reading time by 70-80%\n- **üéØ Focus Enhancement**: Highlights key information and insights\n- **üì± Accessibility**: Markdown format works across all platforms\n- **üîÑ Consistency**: Standardized summary format for all content\n- **ü§ù Shareability**: Easy to share and collaborate on summaries\n- **üîí Privacy Options**: Local processing with Ollama for sensitive content\n\n---\n\n*This project demonstrates practical application of AI, web scraping, and content processing technologies with both cloud and local deployment options.*",
   "id": "c37ebaa1071ec36f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "id": "9c6c2ebc731b9925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!uv pip install selenium beautifulsoup4 webdriver-manager",
   "id": "3a7510629e2a878c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:02.120068Z",
     "start_time": "2025-08-10T06:13:02.114536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# System & Environment\n",
    "# ===========================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display"
   ],
   "id": "613e3e793a3e0599",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Scraping Module",
   "id": "843998359b1338c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:04.216035Z",
     "start_time": "2025-08-10T06:13:04.123580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def _setup_driver(self):\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.set_page_load_timeout(self.timeout)\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Failed to initialize Chrome driver: {e}\")\n",
    "\n",
    "    def _extract_main_content(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'img', 'input', 'button', 'nav', 'footer', 'header']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "\n",
    "        # Try to find main content containers in order of preference\n",
    "        content_selectors = [\n",
    "            'main',\n",
    "            'article',\n",
    "            '[role=\"main\"]',\n",
    "            '.content',\n",
    "            '#content',\n",
    "            '.main-content',\n",
    "            '#main-content'\n",
    "        ]\n",
    "\n",
    "        for selector in content_selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                return content_element.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        # Fallback to body if no main content container found\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return body.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        return soup.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    def crawl(self, url):\n",
    "        if not self.driver:\n",
    "            self._setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "\n",
    "            WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            html_content = self.driver.page_source\n",
    "            main_content = self._extract_main_content(html_content)\n",
    "            return main_content\n",
    "\n",
    "        except TimeoutException:\n",
    "            raise Exception(f\"Timeout while loading {url}\")\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ],
   "id": "be2ea096b1a04fac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:11.254269Z",
     "start_time": "2025-08-10T06:13:11.180839Z"
    }
   },
   "cell_type": "code",
   "source": "from bs4 import BeautifulSoup\nimport requests\n\nclass WebSite:\n    \"\"\"\n    A data class representing a scraped website with its core content and metadata.\n    \n    This class serves as a container for website information extracted during\n    the web scraping process, providing a structured way to store and access\n    webpage data for further processing.\n    \n    Attributes:\n        url (str): The URL of the scraped website\n        title (str): The page title extracted from the HTML <title> tag\n        body (str): The cleaned text content from the webpage body\n        links (List[str]): A list of all hyperlink URLs found on the page\n    \"\"\"\n    \n    def __init__(self, url, title, body, links):\n        \"\"\"\n        Initialize a WebSite object with scraped content.\n        \n        Args:\n            url (str): The URL of the website\n            title (str): The page title\n            body (str): The cleaned body text content\n            links (List[str]): List of hyperlink URLs found on the page\n        \"\"\"\n        self.url = url\n        self.title = title\n        self.body = body\n        self.links = links\n\nclass WebUrlCrawler:\n    \"\"\"\n    A web scraper that extracts content from web pages using HTTP requests and BeautifulSoup.\n    \n    This crawler fetches webpage content, cleans HTML markup, and extracts meaningful\n    text content along with metadata. It's designed for simple, fast content extraction\n    from static web pages without JavaScript rendering requirements.\n    \n    Attributes:\n        headers (dict): HTTP headers used for web requests to mimic browser behavior\n        timeout (int): Request timeout in seconds\n        driver: Placeholder attribute for compatibility (not used in this implementation)\n        headless (bool): Placeholder attribute for compatibility (not used in this implementation)\n    \"\"\"\n    \n    # some websites need to use proper headers when fetching them\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n    }\n\n    def __init__(self, headless=True, timeout=10):\n        \"\"\"\n        Initialize the web crawler with configuration options.\n        \n        Args:\n            headless (bool, optional): Compatibility parameter, not used in this implementation.\n                                      Defaults to True\n            timeout (int, optional): Request timeout in seconds. Defaults to 10\n        \"\"\"\n        self.timeout = timeout\n        self.driver = None\n        self.headless = headless\n\n    def crawl(self, url) -> WebSite:\n        \"\"\"\n        Scrape a webpage and extract its content and metadata.\n        \n        This method performs the following operations:\n        1. Sends an HTTP GET request to the specified URL\n        2. Parses the HTML content using BeautifulSoup\n        3. Extracts the page title\n        4. Cleans the body text by removing scripts, styles, images, and inputs\n        5. Extracts all hyperlinks from the page\n        6. Returns a WebSite object with the processed data\n        \n        Args:\n            url (str): The URL of the webpage to scrape\n            \n        Returns:\n            WebSite: An object containing the scraped website data including\n                    URL, title, cleaned body text, and list of links\n                    \n        Raises:\n            requests.RequestException: If the HTTP request fails\n            BeautifulSoup parsing errors: If HTML parsing fails\n        \"\"\"\n        response = requests.get(url, headers=self.headers)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        title = soup.title.string if soup.title else \"No title found\"\n\n        if soup.body:\n            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n                irrelevant.decompose()\n            body = soup.body.get_text(strip=True, separator='\\n')\n        else:\n            body = \"\"\n\n        links = [link.get('href') for link in soup.find_all('a')]\n        links = [link for link in links if link]\n\n        return WebSite(url, title, body, links)",
   "id": "bbffb4482cf9fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM Client",
   "id": "bde63267e57cc786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:15.246331Z",
     "start_time": "2025-08-10T06:13:15.035323Z"
    }
   },
   "cell_type": "code",
   "source": "from openai import OpenAI\n\nclass LLMClient:\n    \"\"\"\n    A client for interacting with language models through OpenAI's API.\n    \n    This client supports both OpenAI's hosted models and local models via custom base URLs.\n    It provides a simplified interface for generating text responses from language models\n    with system prompts to guide model behavior.\n    \n    Attributes:\n        model (str): The model name to use for text generation\n        openai (OpenAI): The OpenAI client instance for API communication\n    \"\"\"\n    \n    def __init__(self, model, base_url=None):\n        \"\"\"\n        Initialize the LLM client with model configuration.\n        \n        Args:\n            model (str): The model name to use (e.g., 'gpt-4o-mini', 'gpt-3.5-turbo')\n            base_url (str, optional): Custom base URL for local models. If provided,\n                                     the model parameter is used as the API key for\n                                     local model authentication. Defaults to None\n        \"\"\"\n        self.model = model\n        if base_url:\n            self.openai = OpenAI(base_url=base_url, api_key=model)\n        else:\n            self.openai = OpenAI()\n\n    def generate_text(self, user_prompt, system_prompt=\"\") -> str:\n        \"\"\"\n        Generate a text response using the configured language model.\n        \n        This method sends a user prompt along with optional system instructions\n        to the language model and returns the generated response. System prompts\n        are used to guide the model's behavior and response style.\n        \n        Args:\n            user_prompt (str): The user's input message or query\n            system_prompt (str, optional): System instructions to guide the model's\n                                         behavior and response format. Defaults to \"\"\n        \n        Returns:\n            str: The model's generated text response\n            \n        Raises:\n            OpenAIError: If the API request fails or returns an error\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n        response = self.openai.chat.completions.create(\n            model=self.model,\n            messages= messages,\n        )\n        return response.choices[0].message.content",
   "id": "b811ef6a40e9cac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summarization",
   "id": "3e867f7e6d2db52f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:17.266274Z",
     "start_time": "2025-08-10T06:13:17.262941Z"
    }
   },
   "cell_type": "code",
   "source": "def summarize(url, llm_client):\n    \"\"\"\n    Generate an AI-powered summary of a webpage's content.\n    \n    This function combines web scraping and AI text generation to create\n    concise, structured summaries of web content. It extracts the webpage\n    content using the WebUrlCrawler and then processes it through a language\n    model to generate a markdown-formatted summary with a TL;DR section.\n    \n    The function performs the following workflow:\n    1. Scrapes the webpage content using WebUrlCrawler\n    2. Constructs prompts for the AI model with the scraped content\n    3. Generates a summary using the provided LLM client\n    4. Displays the summary in markdown format\n    \n    Args:\n        url (str): The URL of the webpage to summarize\n        llm_client (LLMClient): An initialized LLM client for generating the summary\n        \n    Returns:\n        None: The function displays the summary directly using IPython.display.Markdown\n        \n    Raises:\n        Exception: If web scraping fails (network issues, invalid URL, etc.)\n        OpenAIError: If the AI model request fails or returns an error\n    \"\"\"\n    crawler = WebUrlCrawler()\n    website = crawler.crawl(url)\n\n    system_prompt = \"\"\"You are a web page summarizer that analyzes the content of a provided web page and provides a short and relevant summary. You will also provide a TL;DR at the top. Return your response in markdown.\"\"\"\n    user_prompt = f\"\"\"You are looking at the website titled: {website.title}. The content if the website is as follows: {website.body}. \"\"\"\n\n    summary = llm_client.generate_text(system_prompt=system_prompt, user_prompt=user_prompt)\n    display(Markdown(summary))",
   "id": "66a370d90357edc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summarization with gpt-4o-mini\n",
    "\n"
   ],
   "id": "9656a0a7ef274e96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load open_api_key",
   "id": "d8746efa9815f43e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:30.245140Z",
     "start_time": "2025-08-10T06:13:30.240418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully!\")"
   ],
   "id": "49f454d0180b3900",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configure gpt-4o-mini client",
   "id": "ed60d127d13c0246"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:13:32.724795Z",
     "start_time": "2025-08-10T06:13:32.640001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_open_ai = \"gpt-4o-mini\"\n",
    "open_ai_llm_client = LLMClient(model=model_open_ai)"
   ],
   "id": "5153d5772cedff7f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example",
   "id": "c098df7191bcc838"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T06:14:10.172639Z",
     "start_time": "2025-08-10T06:13:35.691300Z"
    }
   },
   "cell_type": "code",
   "source": "summarize(\"https://en.wikipedia.org/wiki/Marie_Curie\", open_ai_llm_client)",
   "id": "4409a39d24fcdde3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# TL;DR\nMarie Curie was a groundbreaking Polish-French physicist and chemist who won Nobel Prizes in both Physics (1903) and Chemistry (1911) for her pioneering work on radioactivity, including the discovery of the elements polonium and radium. She made significant contributions to medical treatment using radioactive isotopes and remains a symbol of female scientific achievement.\n\n---\n\nMarie Curie, born Maria Salomea Sk≈Çodowska on November 7, 1867, in Warsaw, Poland, was a pioneering scientist known for her research in radioactivity. She was the first woman to win a Nobel Prize and the only person to win Nobel Prizes in two scientific fields: Physics in 1903 (shared with her husband Pierre Curie and Henri Becquerel) and Chemistry in 1911 for her discoveries of polonium and radium.\n\nCurie's academic journey began in Warsaw, where she participated in the clandestine \"Flying University\" due to the restrictions placed on women in higher education. In 1891, she moved to Paris to study at the University of Paris, where she made profound advancements in the understanding of radiation. \n\nThroughout her career, she faced numerous challenges, including sexism in academia and personal tragedies, such as the death of her husband in 1906. Despite these obstacles, Marie Curie established the Curie Institute in Paris and pioneered mobile X-ray units during World War I to aid wounded soldiers.\n\nCurie's work not only transformed the field of physics and chemistry but also laid the groundwork for cancer treatment using radiation. She died on July 4, 1934, from aplastic anemia believed to be linked to her long-term exposure to radiation. Today, she is celebrated as a key figure in science and remains an inspiring role model for women in STEM fields."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summarization with gpt-oss:20b",
   "id": "9206ad4fe37a09cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configure gpt-oss:20b client",
   "id": "76fe0218e6d9af8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T19:57:25.384600Z",
     "start_time": "2025-08-06T19:57:25.362990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_open_ai = \"gpt-oss:20b\"\n",
    "gpt_oss_llm_client = LLMClient(model=model_open_ai, base_url=\"http://localhost:11434/v1\")"
   ],
   "id": "576146eb9d7d0be2",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example",
   "id": "26697afb69bf2bb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T20:01:41.963600Z",
     "start_time": "2025-08-06T19:57:27.221651Z"
    }
   },
   "cell_type": "code",
   "source": "summarize(\"https://en.wikipedia.org/wiki/Marie_Curie\", gpt_oss_llm_client)",
   "id": "fc2c061a04bebb3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "0"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}