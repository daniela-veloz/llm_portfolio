{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üåê WebPage Summarizer\n\nAn intelligent web content summarization tool that extracts and condenses webpage information using advanced AI models.\n\n## üìã Overview\n\nThis project creates concise, structured summaries of web content by leveraging state-of-the-art language models and robust web scraping techniques. The tool supports both cloud-based and local AI models, including OpenAI's GPT-4o-mini and the open-source GPT-OSS:20B model through Ollama, providing flexibility for different deployment scenarios. Perfect for quickly understanding lengthy articles, blog posts, or documentation.\n\n## ‚ú® Key Features\n\n- **ü§ñ Dual AI Models**: Powered by OpenAI's `gpt-4o-mini` and open-source `gpt-oss:20b` through Ollama for high-quality text summarization\n- **üîì Local & Cloud Options**: Choose between cloud-based OpenAI models or run models locally with Ollama\n- **üï∑Ô∏è Advanced Web Scraping**: Uses Selenium to handle both static and dynamic JavaScript-rendered websites\n- **üìù Markdown Output**: Generates clean, formatted summaries in Markdown for easy reading and sharing\n- **üéØ Focused Processing**: Efficiently processes individual webpage URLs without crawling entire sites\n- **‚ö° Multi-Tool Integration**: Combines multiple libraries for robust and reliable content extraction\n\n## üõ†Ô∏è Technology Stack\n\n| Component | Technology | Purpose |\n|-----------|------------|---------|\n| **AI Models** | OpenAI GPT-4o-mini, GPT-OSS:20B | Content summarization |\n| **Web Scraping** | Selenium WebDriver | Dynamic content extraction |\n| **HTML Parsing** | BeautifulSoup | Static content processing |\n| **HTTP Requests** | Python Requests | Basic web requests |\n| **AI Integration** | OpenAI API, Ollama | Model access and inference |\n| **Local AI Runtime** | Ollama | Local model execution |\n| **Language** | Python | Core development |\n\n## üöÄ Installation Requirements\n\n### Ollama Setup\nTo use the GPT-OSS:20B model locally, you need to install Ollama:\n\n1. **Install Ollama**: Visit [ollama.com](https://ollama.com) and download for your platform\n2. **Pull the model**: After installation, run:\n   ```bash\n   ollama pull gpt-oss:20b\n   ```\n3. **Start Ollama service**: The service should start automatically, or run:\n   ```bash\n   ollama serve\n   ```\n\n### Python Dependencies\nInstall required Python packages:\n```bash\npip install selenium beautifulsoup4 webdriver-manager openai requests python-dotenv\n```\n\n## üéØ Project Scope\n\n- ‚úÖ **Single URL Processing**: Focuses on individual webpage content\n- ‚úÖ **Content Extraction**: Handles both static and dynamic web content\n- ‚úÖ **AI Summarization**: Generates intelligent, contextual summaries\n- ‚úÖ **Structured Output**: Provides clean Markdown formatting\n- ‚úÖ **Local & Cloud AI**: Supports both local Ollama and cloud OpenAI models\n- ‚ùå **Site Crawling**: Does not process entire websites or multiple pages\n\n## üèÜ Skill Level\n\n**Beginner-Friendly** - Perfect for developers learning:\n- Web scraping fundamentals\n- AI model integration\n- API consumption\n- Local AI deployment with Ollama\n- Content processing pipelines\n\n## üöÄ Use Cases\n\n- **üì∞ News Article Summaries**: Quickly digest lengthy news articles\n- **üìö Research Papers**: Extract key points from academic content\n- **üìñ Documentation**: Summarize technical documentation\n- **üõçÔ∏è Product Reviews**: Condense detailed product information\n- **üíº Business Reports**: Extract insights from corporate content\n\n## üí° Benefits\n\n- **‚è∞ Time-Saving**: Reduces reading time by 70-80%\n- **üéØ Focus Enhancement**: Highlights key information and insights\n- **üì± Accessibility**: Markdown format works across all platforms\n- **üîÑ Consistency**: Standardized summary format for all content\n- **ü§ù Shareability**: Easy to share and collaborate on summaries\n- **üîí Privacy Options**: Local processing with Ollama for sensitive content\n\n---\n\n*This project demonstrates practical application of AI, web scraping, and content processing technologies with both cloud and local deployment options.*",
   "id": "c37ebaa1071ec36f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "id": "9c6c2ebc731b9925"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:55:23.504967Z",
     "start_time": "2025-08-06T18:55:23.366178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import site\n",
    "!uv pip install selenium beautifulsoup4 webdriver-manager"
   ],
   "id": "5ca157aaa38d0e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /Users/daniela_veloz/Workspace/llm_portfolio/.venv\u001b[0m\r\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:55:25.025035Z",
     "start_time": "2025-08-06T18:55:25.022500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# System & Environment\n",
    "# ===========================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display"
   ],
   "id": "613e3e793a3e0599",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Configuration & Authentication",
   "id": "e8f5e5dc5ee0f5dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:55:27.766246Z",
     "start_time": "2025-08-06T18:55:27.763204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully!\")"
   ],
   "id": "ba367b814da910aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Scraping Module",
   "id": "843998359b1338c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:55:32.994962Z",
     "start_time": "2025-08-06T18:55:32.902794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def _setup_driver(self):\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.set_page_load_timeout(self.timeout)\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Failed to initialize Chrome driver: {e}\")\n",
    "\n",
    "    def _extract_main_content(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'img', 'input', 'button', 'nav', 'footer', 'header']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "\n",
    "        # Try to find main content containers in order of preference\n",
    "        content_selectors = [\n",
    "            'main',\n",
    "            'article',\n",
    "            '[role=\"main\"]',\n",
    "            '.content',\n",
    "            '#content',\n",
    "            '.main-content',\n",
    "            '#main-content'\n",
    "        ]\n",
    "\n",
    "        for selector in content_selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                return content_element.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        # Fallback to body if no main content container found\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return body.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        return soup.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    def crawl(self, url):\n",
    "        if not self.driver:\n",
    "            self._setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "\n",
    "            WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            html_content = self.driver.page_source\n",
    "            main_content = self._extract_main_content(html_content)\n",
    "            return main_content\n",
    "\n",
    "        except TimeoutException:\n",
    "            raise Exception(f\"Timeout while loading {url}\")\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ],
   "id": "be2ea096b1a04fac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:55:38.013371Z",
     "start_time": "2025-08-06T18:55:37.941221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "class WebSite:\n",
    "    def __init__(self, url, title, body, links):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.links = links\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    # some websites need to use proper headers when fetching them\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def crawl(self, url) -> WebSite:\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            body = soup.body.get_text(strip=True, separator='\\n')\n",
    "        else:\n",
    "            body = \"\"\n",
    "\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        links = [link for link in links if link]\n",
    "\n",
    "        return WebSite(url, title, body, links)\n",
    "\n"
   ],
   "id": "bbffb4482cf9fb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM Client",
   "id": "bde63267e57cc786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:56:37.748793Z",
     "start_time": "2025-08-06T18:56:37.745420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model, base_url=None):\n",
    "        self.model = model\n",
    "        if base_url:\n",
    "            self.openai = OpenAI(base_url=base_url, api_key=model)\n",
    "        else:\n",
    "            self.openai = OpenAI()\n",
    "\n",
    "    def generate_text(self, user_prompt, system_prompt=\"\") -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=MODEL_OPENAI,\n",
    "            messages= messages,\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ],
   "id": "b811ef6a40e9cac5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summarization",
   "id": "3e867f7e6d2db52f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:59:37.483036Z",
     "start_time": "2025-08-06T18:59:37.480037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize(url, llm_client):\n",
    "    crawler = WebUrlCrawler()\n",
    "    website = crawler.crawl(url)\n",
    "\n",
    "    system_prompt = \"\"\"You are a web page summarizer that analyzes the content of a provided web page and provides a short and relevant summary. Return your response in markdown.\"\"\"\n",
    "    user_prompt = f\"\"\"You are looking at the website titled: {website.title}. The content if the website is as follows: {website.body}. \"\"\"\n",
    "\n",
    "    print(\"creating summary ...\")\n",
    "\n",
    "    summary = llm_client.generate_text(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "    display(Markdown(summary))"
   ],
   "id": "66a370d90357edc2",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summarization with gpt-4o-mini\n",
    "\n"
   ],
   "id": "9656a0a7ef274e96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T19:01:54.925485Z",
     "start_time": "2025-08-06T19:01:54.905501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_open_ai = \"gpt-4o-mini\"\n",
    "open_ai_llm_client = LLMClient(model=model_open_ai)"
   ],
   "id": "5153d5772cedff7f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example",
   "id": "c098df7191bcc838"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T18:57:12.946234Z",
     "start_time": "2025-08-06T18:57:03.571557Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating summary ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# Marie Curie - Summary\n\nMarie Curie (1867-1934) was a Polish-born physicist and chemist who became a naturalized French citizen. She is best known for her pioneering research on radioactivity, which included the discovery of the elements polonium and radium. Curie was the first woman to win a Nobel Prize and remains the only person to have received Nobel Prizes in two different scientific fields (Physics in 1903 and Chemistry in 1911).\n\nBorn Maria Salomea Sk≈Çodowska in Warsaw, Curie faced significant challenges as a woman in science during her time. Nonetheless, she excelled academically, earning her degrees from the University of Paris while conducting groundbreaking research. She shared her first Nobel Prize with her husband Pierre Curie and Henri Becquerel for their work on radiation phenomena. After Pierre's untimely death in 1906, she became the first female professor at the University of Paris. \n\nCurie's contributions extended beyond academia; during World War I, she developed mobile radiography units to assist in treating wounded soldiers. Her legacy includes not only advancements in science but also significant impacts on medical treatment through the application of radioactivity.\n\nShe passed away from aplastic anemia, likely due to prolonged exposure to radiation during her research. Posthumously, she was recognized for her work and became the first woman interred in the Panth√©on in Paris based on her own merits. Marie Curie's name and contributions remain a symbol of women's achievements in science and continue to inspire countless individuals around the world."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21,
   "source": "summarize(\"https://en.wikipedia.org/wiki/Marie_Curie\", open_ai_llm_client)",
   "id": "4409a39d24fcdde3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summarization with gpt-oss:20b",
   "id": "9206ad4fe37a09cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T19:03:12.050471Z",
     "start_time": "2025-08-06T19:03:12.027977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_open_ai = \"gpt-oss:20b\"\n",
    "gpt_oss_llm_client = LLMClient(model=model_open_ai)"
   ],
   "id": "576146eb9d7d0be2",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example",
   "id": "26697afb69bf2bb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T19:03:24.360695Z",
     "start_time": "2025-08-06T19:03:16.309345Z"
    }
   },
   "cell_type": "code",
   "source": "summarize(\"https://en.wikipedia.org/wiki/Marie_Curie\", gpt_oss_llm_client)",
   "id": "fc2c061a04bebb3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating summary ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# Summary of Marie Curie - Wikipedia\n\nMarie Curie (1867-1934), born Maria Salomea Sk≈Çodowska in Warsaw, was a pioneering Polish-French physicist and chemist best known for her groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize, the only person to win Nobel Prizes in two scientific fields (Physics in 1903 and Chemistry in 1911), and she discovered the elements polonium and radium. \n\nCurie's early life was marked by hardship, including her mother's death from tuberculosis and societal obstacles for women in education. She moved to Paris in 1891 to pursue her studies, where she eventually married fellow scientist Pierre Curie. Together, they discovered radioactivity and advanced the scientific understanding of atomic structures.\n\nDuring World War I, she developed mobile X-ray units to assist in treating wounded soldiers, further demonstrating her commitment to humanitarian causes. Curie died from aplastic anemia, likely due to prolonged radiation exposure during her research work. \n\nHer legacy includes numerous honors and institutions named in her memory. The Curie Institutes in Paris and Warsaw continue her work in medical research. In 1995, she became the first woman to be interred at the Panth√©on in Paris for her own merits, celebrating her significant contributions to science and society."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}