{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üåê WebPage Summarizer\n",
    "\n",
    "An intelligent web content summarization tool that extracts and condenses webpage information using advanced AI models.\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This project creates concise, structured summaries of web content by leveraging state-of-the-art language models and robust web scraping techniques. Perfect for quickly understanding lengthy articles, blog posts, or documentation.\n",
    "\n",
    "## ‚ú® Key Features\n",
    "\n",
    "- **ü§ñ Dual AI Models**: Powered by OpenAI's `gpt-4o-mini` and `llama3.2:3b` for high-quality text summarization\n",
    "- **üï∑Ô∏è Advanced Web Scraping**: Uses Selenium to handle both static and dynamic JavaScript-rendered websites\n",
    "- **üìù Markdown Output**: Generates clean, formatted summaries in Markdown for easy reading and sharing\n",
    "- **üéØ Focused Processing**: Efficiently processes individual webpage URLs without crawling entire sites\n",
    "- **‚ö° Multi-Tool Integration**: Combines multiple libraries for robust and reliable content extraction\n",
    "\n",
    "## üõ†Ô∏è Technology Stack\n",
    "\n",
    "| Component | Technology | Purpose |\n",
    "|-----------|------------|---------|\n",
    "| **AI Models** | OpenAI GPT-4o-mini, Llama 3.2:3b | Content summarization |\n",
    "| **Web Scraping** | Selenium WebDriver | Dynamic content extraction |\n",
    "| **HTML Parsing** | BeautifulSoup | Static content processing |\n",
    "| **HTTP Requests** | Python Requests | Basic web requests |\n",
    "| **AI Integration** | OpenAI API, Ollama | Model access and inference |\n",
    "| **Language** | Python | Core development |\n",
    "\n",
    "## üéØ Project Scope\n",
    "\n",
    "- ‚úÖ **Single URL Processing**: Focuses on individual webpage content\n",
    "- ‚úÖ **Content Extraction**: Handles both static and dynamic web content\n",
    "- ‚úÖ **AI Summarization**: Generates intelligent, contextual summaries\n",
    "- ‚úÖ **Structured Output**: Provides clean Markdown formatting\n",
    "- ‚ùå **Site Crawling**: Does not process entire websites or multiple pages\n",
    "\n",
    "## üèÜ Skill Level\n",
    "\n",
    "**Beginner-Friendly** - Perfect for developers learning:\n",
    "- Web scraping fundamentals\n",
    "- AI model integration\n",
    "- API consumption\n",
    "- Content processing pipelines\n",
    "\n",
    "## üöÄ Use Cases\n",
    "\n",
    "- **üì∞ News Article Summaries**: Quickly digest lengthy news articles\n",
    "- **üìö Research Papers**: Extract key points from academic content\n",
    "- **üìñ Documentation**: Summarize technical documentation\n",
    "- **üõçÔ∏è Product Reviews**: Condense detailed product information\n",
    "- **üíº Business Reports**: Extract insights from corporate content\n",
    "\n",
    "## üí° Benefits\n",
    "\n",
    "- **‚è∞ Time-Saving**: Reduces reading time by 70-80%\n",
    "- **üéØ Focus Enhancement**: Highlights key information and insights\n",
    "- **üì± Accessibility**: Markdown format works across all platforms\n",
    "- **üîÑ Consistency**: Standardized summary format for all content\n",
    "- **ü§ù Shareability**: Easy to share and collaborate on summaries\n",
    "\n",
    "---\n",
    "\n",
    "*This project demonstrates practical application of AI, web scraping, and content processing technologies in a real-world scenario.*"
   ],
   "id": "c37ebaa1071ec36f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "id": "9c6c2ebc731b9925"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:37.228182Z",
     "start_time": "2025-08-06T14:49:37.087270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import site\n",
    "!uv pip install selenium beautifulsoup4 webdriver-manager"
   ],
   "id": "5ca157aaa38d0e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mUsing Python 3.12.11 environment at: /Users/daniela_veloz/Workspace/llm_portfolio/.venv\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m3 packages\u001B[0m \u001B[2min 3ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:40.476528Z",
     "start_time": "2025-08-06T14:49:40.474123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# System & Environment\n",
    "# ===========================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ===========================\n",
    "# AI-related\n",
    "# ===========================\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ],
   "id": "613e3e793a3e0599",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Configuration & Authentication",
   "id": "e8f5e5dc5ee0f5dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:42.768451Z",
     "start_time": "2025-08-06T14:49:42.738947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "   raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully!\")\n",
    "openai = OpenAI()"
   ],
   "id": "ba367b814da910aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:44.799142Z",
     "start_time": "2025-08-06T14:49:44.797127Z"
    }
   },
   "cell_type": "code",
   "source": "MODEL_OPENAI = \"gpt-4o-mini\"",
   "id": "66e9d7beaabf1652",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Web Scraping Module",
   "id": "843998359b1338c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:46.425977Z",
     "start_time": "2025-08-06T14:49:46.420197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def _setup_driver(self):\n",
    "        chrome_options = Options()\n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.driver.set_page_load_timeout(self.timeout)\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Failed to initialize Chrome driver: {e}\")\n",
    "\n",
    "    def _extract_main_content(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        unwanted_tags = ['script', 'style', 'img', 'input', 'button', 'nav', 'footer', 'header']\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "\n",
    "        # Try to find main content containers in order of preference\n",
    "        content_selectors = [\n",
    "            'main',\n",
    "            'article',\n",
    "            '[role=\"main\"]',\n",
    "            '.content',\n",
    "            '#content',\n",
    "            '.main-content',\n",
    "            '#main-content'\n",
    "        ]\n",
    "\n",
    "        for selector in content_selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                return content_element.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        # Fallback to body if no main content container found\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            return body.get_text(strip=True, separator='\\n')\n",
    "\n",
    "        return soup.get_text(strip=True, separator='\\n')\n",
    "\n",
    "    def crawl(self, url):\n",
    "        if not self.driver:\n",
    "            self._setup_driver()\n",
    "\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "\n",
    "            WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "\n",
    "            html_content = self.driver.page_source\n",
    "            main_content = self._extract_main_content(html_content)\n",
    "            return main_content\n",
    "\n",
    "        except TimeoutException:\n",
    "            raise Exception(f\"Timeout while loading {url}\")\n",
    "        except WebDriverException as e:\n",
    "            raise Exception(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ],
   "id": "be2ea096b1a04fac",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:49:56.404273Z",
     "start_time": "2025-08-06T14:49:56.398968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "class WebSite:\n",
    "    def __init__(self, url, title, body, links):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.links = links\n",
    "\n",
    "class WebUrlCrawler:\n",
    "    # some websites need to use proper headers when fetching them\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, headless=True, timeout=10):\n",
    "        self.timeout = timeout\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "\n",
    "    def crawl(self, url) -> WebSite:\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            body = soup.body.get_text(strip=True, separator='\\n')\n",
    "        else:\n",
    "            body = \"\"\n",
    "\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        links = [link for link in links if link]\n",
    "\n",
    "        return WebSite(url, title, body, links)\n",
    "\n"
   ],
   "id": "bbffb4482cf9fb",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM functions",
   "id": "bde63267e57cc786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:50:06.463461Z",
     "start_time": "2025-08-06T14:50:06.459427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def generate_prompt_messages(website:WebSite) -> Any:\n",
    "    system_prompt = \"\"\"You are a web page summarizer that analyzes the content of a provided web page and provides a short and relevant summary. Return your response in markdown.\"\"\"\n",
    "    user_prompt = f\"\"\"You are looking at the website titled: {website.title}. The content if the website is as follows: {website.body}. \"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "def invoke_llm(website:WebSite) -> str:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_OPENAI,\n",
    "        messages=generate_prompt_messages(website),\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "id": "715cb6d5f85aa296",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summarization",
   "id": "3e867f7e6d2db52f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:50:08.773860Z",
     "start_time": "2025-08-06T14:50:08.770817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize(url):\n",
    "    crawler = WebUrlCrawler()\n",
    "    site = crawler.crawl(url)\n",
    "\n",
    "    print(\"creating summary ...\")\n",
    "\n",
    "    summary = invoke_llm(site)\n",
    "    display(Markdown(summary))"
   ],
   "id": "66a370d90357edc2",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:50:21.106397Z",
     "start_time": "2025-08-06T14:50:10.832073Z"
    }
   },
   "cell_type": "code",
   "source": "summarize(\"https://en.wikipedia.org/wiki/Marie_Curie\")",
   "id": "4409a39d24fcdde3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating summary ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "# Summary of Marie Curie - Wikipedia\n\nMarie Curie (1867‚Äì1934), born Maria Salomea Sk≈Çodowska, was a pioneering Polish-French physicist and chemist known for her groundbreaking research on radioactivity. She became the first woman to win a Nobel Prize in 1903 in Physics, shared with her husband Pierre Curie and Henri Becquerel. Additionally, she won a second Nobel Prize in Chemistry in 1911 for her discovery of the elements polonium and radium, making her the first person to win Nobel Prizes in two different scientific fields.\n\nCurie was notable for her work in isolating radioactive isotopes and her development of mobile X-ray machines during World War I, which significantly contributed to medical practices in battlefield conditions. Born in Warsaw, she faced numerous obstacles due to her gender, yet she persevered, eventually becoming the first female professor at the University of Paris. \n\nHer health deteriorated due to prolonged exposure to radiation, leading to her death from aplastic anemia in 1934. Curie's legacy includes the continuing influence of her research in the fields of physics and medicine, several medical institutes named in her honor, and the establishment of the curie as a unit of radioactivity. She remains an iconic figure in science and a symbol of women's contributions to the field.\n\n## Key Points:\n- **Birth:** November 7, 1867, Warsaw, Poland\n- **Death:** July 4, 1934, Passy, France\n- **Nobel Prizes:** Physics (1903), Chemistry (1911)\n- **Notable Discoveries:** Polonium, Radium\n- **Legacy:** Curie unit of radioactivity, Curie Institutes, numerous honors and memorials worldwide."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
